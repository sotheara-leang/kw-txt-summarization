{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.nn.functional as f\n",
    "from random import randint\n",
    "from rouge import Rouge\n",
    "from colr import color\n",
    "\n",
    "from main.common.common import *\n",
    "from main.common.vocab import *\n",
    "from main.common.simple_vocab import SimpleVocab\n",
    "from main.common.util.file_util import FileUtil\n",
    "from main.data.cnn_dataloader import *\n",
    "from main.seq2seq import Seq2Seq\n",
    "from main.common.glove.embedding import GloveEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attention_heatmap_text(words, attn_scores):\n",
    "    c = (255, 165, 0)\n",
    "    \n",
    "    att_words = []\n",
    "    for idx, word in enumerate(words):\n",
    "        attn_score = attn_scores[idx]\n",
    "        attn_color = get_color(c, attn_score)\n",
    "        \n",
    "        att_words.append(color(word, back=attn_color))\n",
    "        \n",
    "    return ' '.join(att_words)\n",
    "\n",
    "def get_color(color, opacity):\n",
    "    r = 255 - opacity * (255 - color[0])\n",
    "    g = 255 - opacity * (255 - color[1])\n",
    "    b = 255 - opacity * (255 - color[2])\n",
    "    return (r, g, b)\n",
    "\n",
    "def show_attention_heatmap(article, summary, attention):\n",
    "    attention = attention[:, :-1]\n",
    "    \n",
    "    # figure\n",
    "    figure = plt.figure(figsize=(20, 5))\n",
    "    ax = figure.add_subplot(111)\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "    figure.colorbar(cax)\n",
    "    \n",
    "    # set up axes\n",
    "    ax.set_xticklabels([''] + article + ['[STOP]'], rotation=90)\n",
    "    ax.set_yticklabels([''] + summary)\n",
    "\n",
    "    # show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def get_score(summary, reference):\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    summary = summary.split()        \n",
    "    summary = [w for w in summary if w != TK_STOP['word']]\n",
    "    \n",
    "    score = rouge.get_scores(' '.join(summary), reference)[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:SimpleVocab:initialize vocabulary from: /home/vivien/PycharmProjects/kw-txt-summarization/data/train/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "AppContext()\n",
    "\n",
    "vocab = SimpleVocab(FileUtil.get_file_path(conf('vocab-file')), conf('vocab-size'))\n",
    "\n",
    "embedding = GloveEmbedding(FileUtil.get_file_path(conf('emb-file')), vocab) if conf('emb-file') is not None else None\n",
    "\n",
    "seq2seq = cuda(Seq2Seq(vocab, embedding))\n",
    "\n",
    "checkpoint = t.load(FileUtil.get_file_path(conf('model-file')))\n",
    "\n",
    "seq2seq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "seq2seq.eval()\n",
    "\n",
    "data_loader =  CNNDataLoader(FileUtil.get_file_path(conf('train:article-file')),\n",
    "                             FileUtil.get_file_path(conf('train:summary-file')),\n",
    "                             FileUtil.get_file_path(conf('train:keyword-file')),\n",
    "                             conf('train:batch-size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> article:  ( cnn ) -- qatar plans to build nine fully air - conditioned open - air stadiums to stage matches at the 2022 fifa world cup . click through the gallery above to see how the stadiums will look .\n",
      ">>> keyword:  \n",
      "========================\n",
      ">>> reference:  qatar hopes to host the 2022 world cup in temperatures of over 40 c . it plans to use solar power to air condition its stadiums . qatar will be the first country in the middle east to stage the event .\n",
      ">>> prediction:  godfather number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number number\n",
      ">>> score:  0.0\n",
      ">>> article with heatmap:  \u001b[48;2;255;165;0m(\u001b[0m \u001b[48;2;255;165;0mcnn\u001b[0m \u001b[48;2;255;165;0m)\u001b[0m \u001b[48;2;255;165;0m--\u001b[0m \u001b[48;2;255;165;0mqatar\u001b[0m \u001b[48;2;255;165;0mplans\u001b[0m \u001b[48;2;255;165;0mto\u001b[0m \u001b[48;2;255;165;0mbuild\u001b[0m \u001b[48;2;255;165;0mnine\u001b[0m \u001b[48;2;255;165;0mfully\u001b[0m \u001b[48;2;255;165;0mair\u001b[0m \u001b[48;2;255;165;0m-\u001b[0m \u001b[48;2;255;165;0mconditioned\u001b[0m \u001b[48;2;255;165;0mopen\u001b[0m \u001b[48;2;255;165;0m-\u001b[0m \u001b[48;2;255;165;0mair\u001b[0m \u001b[48;2;255;165;0mstadiums\u001b[0m \u001b[48;2;255;165;0mto\u001b[0m \u001b[48;2;255;165;0mstage\u001b[0m \u001b[48;2;255;165;0mmatches\u001b[0m \u001b[48;2;255;165;0mat\u001b[0m \u001b[48;2;255;165;0mthe\u001b[0m \u001b[48;2;255;165;0m2022\u001b[0m \u001b[48;2;255;165;0mfifa\u001b[0m \u001b[48;2;255;165;0mworld\u001b[0m \u001b[48;2;255;165;0mcup\u001b[0m \u001b[48;2;255;165;0m.\u001b[0m \u001b[48;2;255;165;0mclick\u001b[0m \u001b[48;2;255;165;0mthrough\u001b[0m \u001b[48;2;255;165;0mthe\u001b[0m \u001b[48;2;255;165;0mgallery\u001b[0m \u001b[48;2;255;165;0mabove\u001b[0m \u001b[48;2;255;165;0mto\u001b[0m \u001b[48;2;255;165;0msee\u001b[0m \u001b[48;2;255;165;0mhow\u001b[0m \u001b[48;2;255;165;0mthe\u001b[0m \u001b[48;2;255;165;0mstadiums\u001b[0m \u001b[48;2;255;165;0mwill\u001b[0m \u001b[48;2;255;165;0mlook\u001b[0m \u001b[48;2;255;165;0m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "samples = data_loader.read_all()\n",
    "\n",
    "article, keyword, reference = samples[3]\n",
    "\n",
    "keyword = keyword[0]\n",
    "reference = reference[0]\n",
    "    \n",
    "summary, attention = seq2seq.evaluate(article, keyword)\n",
    "\n",
    "score = get_score(summary, reference)\n",
    "\n",
    "print('>>> article: ', article)\n",
    "print('>>> keyword: ', keyword)\n",
    "print('========================')\n",
    "print('>>> reference: ', reference)\n",
    "print('>>> prediction: ', summary)\n",
    "print('>>> score: ', score)\n",
    "\n",
    "article_words = article.split()\n",
    "summary_words = summary.split()\n",
    "attention = attention.cpu()\n",
    "\n",
    "#show_attention_heatmap(article_words, summary_words, attention)\n",
    "\n",
    "article_words_attention = t.sum(attention, dim=0) / t.max(attention)\n",
    "article_words_attention = t.clamp(article_words_attention, 0, 1)\n",
    "\n",
    "heatmap_text = generate_attention_heatmap_text(article_words, article_words_attention)\n",
    "\n",
    "print('>>> article with heatmap: ', heatmap_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
